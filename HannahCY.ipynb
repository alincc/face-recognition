{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<!-- This cell makes the font bigger to make it easy to read. Adjust to taste -->\n",
       "<style>\n",
       ".cell, .CodeMirror pre{ \n",
       "    font-size: 150%;\n",
       "    line-height: 125%;\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%html\n",
    "<!-- This cell makes the font bigger to make it easy to read. Adjust to taste -->\n",
    "<style>\n",
    ".cell, .CodeMirror pre{ \n",
    "    font-size: 150%;\n",
    "    line-height: 125%;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COSC470 Assignment 2, 2018\n",
    "\n",
    "## Name: Hannah Clark-Younger\n",
    "## Due Date: Monday September 24th\n",
    "\n",
    "For assignment 2 you need to implement machine learning algorithm(s) to label faces according to:\n",
    "- sex (male/female)\n",
    "- age (child/teen/adult/senior)\n",
    "- expression (smiling/serious)\n",
    "\n",
    "A data set from MIT is made available, along with code to read the images and labels into `numpy` arrays. \n",
    "These arrays are divided into training, validation, and testing data sets.\n",
    "\n",
    "You may use any machine learning algorithms you like to classify the faces.\n",
    "Techniques you may find useful that we've looked at include:\n",
    "- Decision trees and random forests\n",
    "- Boosting (and AdaBoost in particular)\n",
    "- Support Vector Machines (SVMs)\n",
    "- Face detection (to focus on the key parts of the image)\n",
    "- EigenFaces (for dimensionality reduction)\n",
    "- Neural networks in TensorFlow\n",
    "- CNNs in TensorFlow\n",
    "\n",
    "## Submission Requirements\n",
    "\n",
    "You should submit a version of this Notebook renamed to `YourName.ipynb`, so my submission would be `StevenMills.ipynb`. \n",
    "You can assume that the same libraries that are available in the COSC470 Anaconda environment on the lab machines are available.\n",
    "In particular, you can use numpy, scipy, OpenCV, and TensorFlow.\n",
    "\n",
    "I should be able to open your Notebook and run it. The Notebook should contain the code to construct and train your classifier(s) from the training data (using the validation data appropriately) and then to compute the labels of the training data through a call to `computeLabels`, which has a stub implementation at the end of this notebook. \n",
    "\n",
    "## Marking Scheme\n",
    "\n",
    "A rough marking scheme is given below. This is intentionally fairly open, so that I can give you marks for doing good stuff without having to predetermine what stuff is good.\n",
    "\n",
    "- 10 marks for the discussion of choice of algorithms and training strategy\n",
    "- 10 marks for the explanation and clear implementation\n",
    "- 5 marks for performance\n",
    "\n",
    "### Algorithm Choice and Training\n",
    "\n",
    "I will be looking for a description of the algorithm(s) chosen, why you chose that approach, and how you developed, trained and evaluated your method.\n",
    "You should think about issues such as how to best make use of the training and validation data and how to select parameters for your chosen method.\n",
    "\n",
    "You are not restricted to a single classifier or method. If you find it useful to determine age labels first and then use that to help determine expression, then that is fine. If you want to use an SVM for sex classification, but a boosted classifier for age, that's also fine.\n",
    "However, you should discuss why you chose to use the methods you have chosen.\n",
    "\n",
    "### Explanation and Clear Implementation\n",
    "\n",
    "You should implement your chosen algorithm(s) using the training and validation data sets provided. \n",
    "Jupyter notebooks let you interleave discussion and code, so you should clearly describe how your implementation works.\n",
    "You can include mathematics if needed using \\\\(\\LaTeX\\\\)-style markup as demonstrated in the lecture notebooks.\n",
    "I'll be looking for clear implementations that illustrate good practice in training and evaluation.\n",
    "\n",
    "It is expected that you will make appropriate use of libraries such as OpenCV and TensorFlow where appropriate, but your explanation should your understanding of these tools clear. \n",
    "For example, if you choose to use a convolutional network, you should explain your architecture, how it relates to the code, and give some justification for the various parameters that you need to select when making a CNN.\n",
    "\n",
    "### Performance\n",
    "\n",
    "The last cell of the notebook has a function that takes a face data set and produces labels as a result.\n",
    "You should modify this so that it uses your machine learning algorithms to generate the labels.\n",
    "I will then use these labels to compare your results to the ground truth.\n",
    "I may also shuffle the training, validation, and testing data sets around before running your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Data Set\n",
    "\n",
    "The following code reads the data into training, testing, and validation sets.\n",
    "It assumes that the `.zip` of labelled face data set from the course website has been unzipped into the same directory as the notebook.\n",
    "There are 1997 training images, and 998 each test and training images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Read in training data and labels\n",
    "\n",
    "# Some useful parsing functions\n",
    "\n",
    "# male/female -> 0/1\n",
    "def parseSexLabel(string):\n",
    "    if (string.startswith('male')):\n",
    "        return 0\n",
    "    if (string.startswith('female')):\n",
    "        return 1\n",
    "    print(\"ERROR parsing sex from \" + string)\n",
    "\n",
    "\n",
    "# child/teen/adult/senior -> 0/1/2/3\n",
    "def parseAgeLabel(string):\n",
    "    if (string.startswith('child')):\n",
    "        return 0\n",
    "    if (string.startswith('teen')):\n",
    "        return 1\n",
    "    if (string.startswith('adult')):\n",
    "        return 2\n",
    "    if (string.startswith('senior')):\n",
    "        return 3\n",
    "    print(\"ERROR parsing age from \" + string)\n",
    "\n",
    "\n",
    "# serious/smiling -> 0/1\n",
    "def parseExpLabel(string):\n",
    "    if (string.startswith('serious')):\n",
    "        return 0\n",
    "    if (string.startswith('smiling') or string.startswith('funny')):\n",
    "        return 1\n",
    "    print(\"ERROR parsing expression from \" + string)\n",
    "\n",
    "\n",
    "# Count number of training instances\n",
    "\n",
    "numTraining = 0\n",
    "\n",
    "for line in open(\"MITFaces/faceDR\"):\n",
    "    if line.find('_missing descriptor') < 0:\n",
    "        numTraining += 1\n",
    "\n",
    "dimensions = 128 * 128\n",
    "\n",
    "trainingFaces = np.zeros([numTraining, dimensions])\n",
    "trainingSexLabels = np.zeros(numTraining)  # Sex - 0 = male; 1 = female\n",
    "trainingAgeLabels = np.zeros(numTraining)  # Age - 0 = child; 1 = teen; 2 = male\n",
    "trainingExpLabels = np.zeros(numTraining)  # Expression - 0 = serious; 1 = smiling\n",
    "\n",
    "index = 0\n",
    "for line in open(\"MITFaces/faceDR\"):\n",
    "    if line.find('_missing descriptor') >= 0:\n",
    "        continue\n",
    "    # Parse the label data\n",
    "    parts = line.split()\n",
    "    trainingSexLabels[index] = parseSexLabel(parts[2])\n",
    "    trainingAgeLabels[index] = parseAgeLabel(parts[4])\n",
    "    trainingExpLabels[index] = parseExpLabel(parts[8])\n",
    "    # Read in the face\n",
    "    fileName = \"MITFaces/rawdata/\" + parts[0]\n",
    "    fileIn = open(fileName, 'rb')\n",
    "    trainingFaces[index, :] = np.fromfile(fileIn, dtype=np.uint8, count=dimensions) / 255.0\n",
    "    fileIn.close()\n",
    "    # And move along\n",
    "    index += 1\n",
    "\n",
    "# Count number of validation/testing instances\n",
    "\n",
    "numValidation = 0\n",
    "numTesting = 0\n",
    "\n",
    "# Assume they're all Validation\n",
    "for line in open(\"MITFaces/faceDS\"):\n",
    "    if line.find('_missing descriptor') < 0:\n",
    "        numTraining += 1\n",
    "    numValidation += 1\n",
    "\n",
    "# And make half of them testing\n",
    "numTesting = int(numValidation / 2)\n",
    "numValidation -= numTesting\n",
    "\n",
    "validationFaces = np.zeros([numValidation, dimensions])\n",
    "validationSexLabels = np.zeros(numValidation)  # Sex - 0 = male; 1 = female\n",
    "validationAgeLabels = np.zeros(numValidation)  # Age - 0 = child; 1 = teen; 2 = male\n",
    "validationExpLabels = np.zeros(numValidation)  # Expression - 0 = serious; 1 = smiling\n",
    "\n",
    "testingFaces = np.zeros([numTesting, dimensions])\n",
    "testingSexLabels = np.zeros(numTesting)  # Sex - 0 = male; 1 = female\n",
    "testingAgeLabels = np.zeros(numTesting)  # Age - 0 = child; 1 = teen; 2 = male\n",
    "testingExpLabels = np.zeros(numTesting)  # Expression - 0 = serious; 1 = smiling\n",
    "\n",
    "index = 0\n",
    "for line in open(\"MITFaces/faceDS\"):\n",
    "    if line.find('_missing descriptor') >= 0:\n",
    "        continue\n",
    "\n",
    "    # Parse the label data\n",
    "    parts = line.split()\n",
    "    if (index < numTesting):\n",
    "        testingSexLabels[index] = parseSexLabel(parts[2])\n",
    "        testingAgeLabels[index] = parseAgeLabel(parts[4])\n",
    "        testingExpLabels[index] = parseExpLabel(parts[8])\n",
    "        # Read in the face\n",
    "        fileName = \"MITFaces/rawdata/\" + parts[0]\n",
    "        fileIn = open(fileName, 'rb')\n",
    "        testingFaces[index, :] = np.fromfile(fileIn, dtype=np.uint8, count=dimensions) / 255.0\n",
    "        fileIn.close()\n",
    "    else:\n",
    "        vIndex = index - numTesting\n",
    "        validationSexLabels[vIndex] = parseSexLabel(parts[2])\n",
    "        validationAgeLabels[vIndex] = parseAgeLabel(parts[4])\n",
    "        validationExpLabels[vIndex] = parseExpLabel(parts[8])\n",
    "        # Read in the face\n",
    "        fileName = \"MITFaces/rawdata/\" + parts[0]\n",
    "        fileIn = open(fileName, 'rb')\n",
    "        validationFaces[vIndex, :] = np.fromfile(fileIn, dtype=np.uint8, count=dimensions) / 255.0\n",
    "        fileIn.close()\n",
    "\n",
    "    # And move along\n",
    "    index += 1\n",
    "print(\"Data loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONVNET FOR FACE CLASSIFICATION\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-training (Epoch 0) --- Training Accuracy:  0.00%, Validation Accuracy: 75.00%,  Validation Loss: 0.560\n",
      "Training Epoch 1 --- Training Accuracy: 61.09%, Validation Accuracy: 68.75%,  Validation Loss: 0.639\n",
      "Training Epoch 2 --- Training Accuracy: 71.88%, Validation Accuracy: 56.25%,  Validation Loss: 0.877\n",
      "Training Epoch 3 --- Training Accuracy: 75.86%, Validation Accuracy: 68.75%,  Validation Loss: 0.476\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import cv2\n",
    "import scipy\n",
    "import random\n",
    "\n",
    "####### MODIFIABLE PARAMETERS ######   \n",
    "\n",
    "task = \"Sex\" # Options are \"Sex\", \"Age\", \"Expression\"\n",
    "\n",
    "batch_size = 16\n",
    "n_epochs = 1000\n",
    "\n",
    "n_filters_conv1 = 64\n",
    "filter_size_conv1 = 3\n",
    "n_filters_conv2 = 128\n",
    "filter_size_conv2 = 3\n",
    "n_filters_conv3 = 256\n",
    "filter_size_conv3 = 3\n",
    "fc_layer_size = 512\n",
    "\n",
    "display_step = 1\n",
    "saver_step = 10\n",
    "\n",
    "####################################\n",
    "\n",
    "def make_one_hot(labels):\n",
    "    global n_classes\n",
    "    one_label = np.zeros(n_classes)\n",
    "    new_labels = [one_label]*len(labels)\n",
    "    for i in range(len(labels)):\n",
    "        #print(i)\n",
    "        one_label = np.zeros(n_classes)\n",
    "        one_label[int(labels[i])] = 1\n",
    "        new_labels[i] = one_label\n",
    "        #print(labels[i])\n",
    "        #print(new_labels[i])\n",
    "    return np.array(new_labels)\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self,data,labels):\n",
    "        #print(data.shape)\n",
    "        self.data = data.reshape([-1,128,128,1]) #tf.convert_to_tensor(data.reshape([-1,128,128,1]), dtype=tf.float32)\n",
    "        self.labels = labels #.reshape(-1,n_classes) # n_classes\n",
    "        self.batch_index = 0\n",
    "        \n",
    "    def randomize(self, sess):\n",
    "        shuffled_data = np.empty(self.data.shape, dtype=self.data.dtype)\n",
    "        shuffled_labels = np.empty(self.labels.shape, dtype=self.labels.dtype)\n",
    "        permutation = np.random.permutation(len(self.data))\n",
    "        for old_index, new_index in enumerate(permutation):\n",
    "            shuffled_data[new_index] = self.data[old_index]\n",
    "            shuffled_labels[new_index] = self.labels[old_index]\n",
    "        self.data = shuffled_data\n",
    "        self.labels = shuffled_labels\n",
    "    \n",
    "    def next_batch(self, b_size):\n",
    "        start = self.batch_index\n",
    "        end = self.batch_index + b_size\n",
    "        self.batch_index = end\n",
    "        return self.data[start:end], self.labels[start:end]\n",
    "    \n",
    "def conv_pool_relu_layer(input, n_input, n_filters, filter_size):  \n",
    "    weights = tf.Variable(tf.truncated_normal(shape=[filter_size, filter_size, n_input, n_filters], stddev=0.05))\n",
    "    biases = tf.Variable(tf.constant(0.05, shape=[n_filters]))\n",
    "    conv_layer = tf.nn.conv2d(input=input, filter=weights, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    conv_layer += biases\n",
    "    c_m_layer = tf.nn.max_pool(value=conv_layer, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    c_m_r_layer = tf.nn.relu(c_m_layer)\n",
    "    return c_m_r_layer\n",
    "\n",
    "def flat_layer(input_layer):\n",
    "    shape = input_layer.get_shape()\n",
    "    num_features = shape[1:4].num_elements()\n",
    "    flat_layer = tf.reshape(input_layer, [-1, num_features])\n",
    "    return flat_layer\n",
    "\n",
    "def fc_layer(input, n_inputs, n_outputs, use_relu=True):\n",
    "    weights = tf.Variable(tf.truncated_normal(shape=[n_inputs, n_outputs], stddev=0.05))\n",
    "    biases = tf.Variable(tf.constant(0.05, shape=[n_outputs]))\n",
    "    fc_layer = tf.matmul(input, weights) + biases\n",
    "    if use_relu:\n",
    "        fc_layer = tf.nn.relu(fc_layer)\n",
    "    return fc_layer\n",
    "\n",
    "#print(\"after layer defns, before model defined\")\n",
    "\n",
    "if task == \"Sex\":\n",
    "    n_classes = 2\n",
    "    train_labels = make_one_hot(trainingSexLabels)\n",
    "    valid_labels = make_one_hot(validationSexLabels)\n",
    "    test_labels = make_one_hot(testingSexLabels)\n",
    "    train_data = Dataset(trainingFaces,train_labels)\n",
    "    valid_data = Dataset(validationFaces,valid_labels)\n",
    "    test_data = Dataset(testingFaces,test_labels)\n",
    "    model = \"sex-model\"\n",
    "elif task == \"Age\":\n",
    "    n_classes = 4\n",
    "    train_labels = make_one_hot(trainingAgeLabels)\n",
    "    valid_labels = make_one_hot(validationAgeLabels)\n",
    "    test_labels = make_one_hot(testingAgeLabels)\n",
    "    train_data = Dataset(trainingFaces,train_labels)\n",
    "    valid_data = Dataset(validationFaces,valid_labels)\n",
    "    test_data = Dataset(testingFaces,test_labels)\n",
    "    model = \"age-model\"\n",
    "elif task == \"Expression\":\n",
    "    n_classes = 2 \n",
    "    train_labels = make_one_hot(trainingExpLabels)\n",
    "    valid_labels = make_one_hot(validationExpLabels)\n",
    "    test_labels = make_one_hot(testingExpLabels)\n",
    "    train_data = Dataset(trainingFaces,train_labels)\n",
    "    valid_data = Dataset(validationFaces,valid_labels)\n",
    "    test_data = Dataset(testingFaces,test_labels)\n",
    "    model = \"exp-model\"\n",
    "else:\n",
    "    print(\"Please set task to one of the three options\")\n",
    "\n",
    "img_size = 128\n",
    "num_channels = 1 # greyscale (I think)\n",
    "n_batches = trainingFaces.shape[0]//batch_size\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "    # set up basic (change to GoogLeNet?) model\n",
    "    g = tf.Graph()\n",
    "    with g.as_default():\n",
    "        X = tf.placeholder(tf.float32, shape=[None, img_size, img_size, num_channels], name='X')\n",
    "        #print(X.shape)\n",
    "        y_true = tf.placeholder(tf.float32, shape=[None,n_classes], name='y_true') # None, 1 OR n_classes\n",
    "        y_true_class = tf.argmax(y_true, dimension=1)\n",
    "        conv1 = conv_pool_relu_layer(input=X, n_input=num_channels, n_filters=n_filters_conv1, filter_size=filter_size_conv1)\n",
    "        conv2 = conv_pool_relu_layer(input=conv1, n_input=n_filters_conv1, n_filters=n_filters_conv2, filter_size=filter_size_conv2) \n",
    "        conv3 = conv_pool_relu_layer(input=conv2, n_input=n_filters_conv2, n_filters=n_filters_conv3,filter_size=filter_size_conv3)        \n",
    "        flat = flat_layer(conv3)\n",
    "        fc1 = fc_layer(input=flat,n_inputs=flat.get_shape()[1:4].num_elements(),n_outputs=fc_layer_size)\n",
    "        fc2 = fc_layer(input=fc1,n_inputs=fc_layer_size,n_outputs=n_classes,use_relu=False) # n_outputs=n_classes\n",
    "        y_pred = tf.nn.softmax(fc2,name=\"y_pred\")\n",
    "        y_pred_class = tf.argmax(y_pred, dimension=1)\n",
    "        cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=fc2,labels=y_true)\n",
    "        cost = tf.reduce_mean(cross_entropy)\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=1e-4).minimize(cost)\n",
    "        correct_prediction = tf.equal(y_pred_class, y_true_class)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32),name=\"accuracy\")\n",
    "        #print(\"Graph initialised\")\n",
    "\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        #session run with one kind of label\n",
    "        with tf.Session() as sess:\n",
    "            #print(\"inside session\")\n",
    "            # Run the initializer\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            #print(\"Initialised\")\n",
    "            x_valid_batch, y_valid_batch = valid_data.next_batch(batch_size)\n",
    "            feed_dict_val = {X: x_valid_batch, y_true: y_valid_batch}\n",
    "            val_acc = sess.run(accuracy,feed_dict=feed_dict_val)\n",
    "            val_loss = sess.run(cost, feed_dict=feed_dict_val)   \n",
    "            msg = \"Pre-training (Epoch {0}) --- Training Accuracy: {1:>6.2%}, Validation Accuracy: {2:>6.2%},  Validation Loss: {3:.3f}\"\n",
    "            print(msg.format(0, 0, val_acc, val_loss)) # , val_loss))\n",
    "            for i in range(1,n_epochs+1):\n",
    "                train_data.randomize(sess)\n",
    "                train_data.batch_index = 0    \n",
    "                valid_data.randomize(sess)\n",
    "                valid_data.batch_index = 0\n",
    "                acc = 0\n",
    "                for batch in range (n_batches):\n",
    "                    #if batch % 10 == 0:\n",
    "                    #    print('Batch', batch, 'of', n_batches, 'done')\n",
    "                    x_batch, y_true_batch = train_data.next_batch(batch_size)\n",
    "                    feed_dict_train = {X: x_batch, y_true: y_true_batch}\n",
    "                    sess.run(optimizer, feed_dict=feed_dict_train)\n",
    "                    acc += sess.run(accuracy, feed_dict=feed_dict_train)\n",
    "                acc = acc/n_batches\n",
    "                if i % display_step == 0: \n",
    "                    valid_data.batch_index = 0\n",
    "                    x_valid_batch, y_valid_batch = valid_data.next_batch(batch_size)\n",
    "                    feed_dict_val = {X: x_valid_batch, y_true: y_valid_batch}\n",
    "                    #val_loss = sess.run(cost, feed_dict=feed_dict_val)\n",
    "                    val_acc = sess.run(accuracy,feed_dict=feed_dict_val)\n",
    "                    val_loss = sess.run(cost, feed_dict=feed_dict_val)\n",
    "                    #epoch = int(i / int(data.train.num_examples/batch_size))   \n",
    "                    msg = \"Training Epoch {0} --- Training Accuracy: {1:>6.2%}, Validation Accuracy: {2:>6.2%},  Validation Loss: {3:.3f}\"\n",
    "                    print(msg.format(i, acc, val_acc, val_loss)) # , val_loss))\n",
    "                if i % saver_step == 0 or val_acc > 0.9:\n",
    "                    save_path = saver.save(sess, model+\"_\"+str(i))\n",
    "\n",
    "print(\"Done!\")                \n",
    "#print(numTraining)\n",
    "#print(numValidation)\n",
    "#print(numTesting)\n",
    "#print(trainingFaces.shape)\n",
    "#print(validationFaces.shape)\n",
    "#print(testingFaces.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will be used to evaluate your submission.\n",
    "\n",
    "def computeLabels(faceData):\n",
    "    n, d = faceData.shape\n",
    "    # Zero arrays for the labels, should be able to do better than this\n",
    "    estSexLabels = np.zeros(n)\n",
    "    estAgeLabels = np.zeros(n)\n",
    "    estExpLabels = np.zeros(n)\n",
    "    return estSexLabels, estAgeLabels, estExpLabels\n",
    "\n",
    "estS, estA, estE = computeLabels(validationFaces)\n",
    "# I'll do stuff with the above to evaluate the accuracy of your methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
